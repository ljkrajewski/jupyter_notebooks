{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljkrajewski/jupyter_notebooks/blob/main/flux/GUFlux.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grand Unified Flux\n",
        "With much love to @camenduru for the [flux_jupyter](https://github.com/camenduru/flux-jupyter) repository."
      ],
      "metadata": {
        "id": "Dm3lnv1tYjwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install prerequisits and restart the session\n",
        "#@markdown (Colab will report a system crash. _Don't Panic!!_)\n",
        "import IPython\n",
        "\n",
        "!pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q torchsde einops diffusers accelerate gradio==3.50.2 python-multipart==0.0.12\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "# Install torchvision with CUDA support\n",
        "!pip uninstall torch torchvision -y\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "print(\"\\nRestarting session...\")\n",
        "IPython.get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tdMZmIsyL31t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After the restart, run the following cells."
      ],
      "metadata": {
        "id": "rKNGWrU4veVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Connect Google Drive\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
        "Connect_Google_drive = False #@param {type:\"boolean\"}\n",
        "#@markdown (optional) Leave \"Directory_name\" blank for default directory name.\n",
        "Directory_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if Connect_Google_drive:\n",
        "  print(\"Connecting...\")\n",
        "  drive.mount('/content/gdrive')\n",
        "  mainpth=\"/content/gdrive/MyDrive\"\n",
        "else:\n",
        "  mainpth=\"/content\"\n",
        "\n",
        "if Directory_name == \"\":\n",
        "  now = datetime.now()\n",
        "  timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "  Directory_name = f\"flux-{timestamp}\"\n",
        "\n",
        "if not os.path.exists(f'{mainpth}/{Directory_name}'):\n",
        "  %mkdir -p $mainpth/flux-$Directory_name\n",
        "picture_path = f'{mainpth}/{Directory_name}'\n",
        "\n",
        "picture_path"
      ],
      "metadata": {
        "id": "EG6QMeImIryP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download code and models\n",
        "%cd /content\n",
        "!git clone -b totoro4 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8-all-in-one.safetensors -d /content/TotoroUI/models/checkpoints -o flux1-dev-fp8-all-in-one.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux_realism_lora.safetensors -d /content/TotoroUI/models/loras -o flux_realism_lora.safetensors"
      ],
      "metadata": {
        "id": "SCcSaVy9aS3u",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "#@title Define local routines and start gradio\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro_extras import nodes_flux\n",
        "from totoro import model_management\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "from google.colab import runtime\n",
        "import gradio as gr\n",
        "import os\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "def add_ai_metadata(image_path, prompt, seed, steps, guidance, sampler_name, scheduler, lora_strength_model, lora_strength_clip):\n",
        "    \"\"\"\n",
        "    Adds metadata related to a stable diffusion image generation to a PNG image file.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the PNG image file.\n",
        "        prompt: The text prompt used for image generation.\n",
        "        seed: The random seed used for image generation.\n",
        "        steps: The number of denoising steps.\n",
        "        guidance: The classifier-free guidance scale (cfg_scale).\n",
        "        sampler_name: The name of the sampler used.\n",
        "        scheduler: The scheduler used.\n",
        "        lora_strength_model: The strength of the LoRA model.\n",
        "        lora_strength_clip: The strength of the LoRA CLIP.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        png_info = PngInfo()\n",
        "\n",
        "        png_info.add_text(\"Prompt\", prompt)\n",
        "        png_info.add_text(\"Seed\", str(seed))\n",
        "        png_info.add_text(\"Steps\", str(steps))\n",
        "        png_info.add_text(\"Guidance\", str(guidance))\n",
        "        png_info.add_text(\"Sampler\", sampler_name)\n",
        "        png_info.add_text(\"Scheduler\", scheduler)\n",
        "        png_info.add_text(\"LoRA\", \"flux_realism_lora\")\n",
        "        png_info.add_text(\"LoRA Strength Model\", str(lora_strength_model))\n",
        "        png_info.add_text(\"LoRA Strength CLIP\", str(lora_strength_clip))\n",
        "        img.save(image_path, pnginfo=png_info)\n",
        "        print(f\"Metadata added successfully to {image_path}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def optimal_dimensions(wh_ratio):\n",
        "    \"\"\"\n",
        "    Calculates optimal dimensions for Stable Diffusion.\n",
        "\n",
        "    Args:\n",
        "      wh_ratio (str): Width-to-height ratio in the format \"width:height\".\n",
        "\n",
        "    Returns:\n",
        "      A tuple of (new_width, new_height) representing the optimal dimensions.\n",
        "    \"\"\"\n",
        "    sw, sh = wh_ratio.split(':')\n",
        "    w, h = int(sw), int(sh)\n",
        "    c = math.sqrt(1024**2 / (w * h))\n",
        "    new_width = int(((w * c) // 16) * 16)\n",
        "    new_height = int(((h * c) // 16) * 16)\n",
        "    #print(f\"Optimal dimensions: {new_width}x{new_height}\")\n",
        "    return new_width, new_height\n",
        "\n",
        "CheckpointLoaderSimple = NODE_CLASS_MAPPINGS[\"CheckpointLoaderSimple\"]()\n",
        "LoraLoader = NODE_CLASS_MAPPINGS[\"LoraLoader\"]()\n",
        "FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    unet, clip, vae = CheckpointLoaderSimple.load_checkpoint(\"flux1-dev-fp8-all-in-one.safetensors\")\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(positive_prompt, wh_ratio, orientation, seed, steps, sampler_name, scheduler, guidance, lora_strength_model, lora_strength_clip):\n",
        "    global unet, clip\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(f\"\\nSeed:  {seed}\")\n",
        "    width, height = optimal_dimensions(wh_ratio)\n",
        "    if orientation == \"portrait\":\n",
        "        width, height = height, width\n",
        "    print(f\"Dimentions:  {width}x{height} ({orientation})\")\n",
        "    unet_lora, clip_lora = LoraLoader.load_lora(unet, clip, \"flux_realism_lora.safetensors\", lora_strength_model, lora_strength_clip)\n",
        "    cond, pooled = clip_lora.encode_from_tokens(clip_lora.tokenize(positive_prompt), return_pooled=True)\n",
        "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "    cond = FluxGuidance.append(cond, guidance)[0]\n",
        "    noise = RandomNoise.get_noise(seed)[0]\n",
        "    guider = BasicGuider.get_guider(unet_lora, cond)[0]\n",
        "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "    sigmas = BasicScheduler.get_sigmas(unet_lora, scheduler, steps, 1.0)[0]\n",
        "    latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "    sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(f\"{picture_path}/flux.png\")\n",
        "    add_ai_metadata(f\"{picture_path}/flux.png\", positive_prompt, seed, steps, guidance, sampler_name, scheduler, lora_strength_model, lora_strength_clip)\n",
        "    return f\"{picture_path}/flux.png\"\n",
        "\n",
        "def round_robin_prompts(prompt):\n",
        "    \"\"\"\n",
        "    Generates all possible permutations of a prompt with round-robin sections.\n",
        "\n",
        "    Args:\n",
        "        prompt: A natural language prompt containing round-robin sections\n",
        "                enclosed in curly braces with items separated by '|'.\n",
        "                Example: \"a {red|blue} cup on a {table|chair}\"\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string is a possible permutation of the prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    sections = []\n",
        "    split_prompt = re.split(r'({.*?})', prompt)\n",
        "\n",
        "    for part in split_prompt:\n",
        "        if part.startswith('{') and part.endswith('}'):\n",
        "            items = part[1:-1].split('|')\n",
        "            sections.append(items)\n",
        "        else:\n",
        "            sections.append([part])  # Non-round-robin parts are treated as single-item sections\n",
        "\n",
        "    combinations = list(itertools.product(*sections))\n",
        "    result = []\n",
        "    for combo in combinations:\n",
        "        result.append(\"\".join(combo))\n",
        "    return result\n",
        "\n",
        "def generate_wrapper(positive_prompt, quantity, wh_ratio, orientation, seed, steps, sampler_name, scheduler, guidance, lora_strength_model, lora_strength_clip):\n",
        "    prompts = round_robin_prompts(positive_prompt)\n",
        "    num_prompts = len(prompts)\n",
        "    for p_idx, prompt in enumerate(prompts):\n",
        "        #print(f\"Prompt:  {prompt}\")\n",
        "        for i in range(quantity):\n",
        "            print(f\"----\\nGenerating image {i+1}/{quantity}, prompt {p_idx+1}/{num_prompts}...\")\n",
        "            generate(prompt, wh_ratio, orientation, seed, steps, sampler_name, scheduler, guidance, lora_strength_model, lora_strength_clip)\n",
        "            new_image_name=f\"{picture_path}/flux{p_idx+1}-{i+1}.png\"\n",
        "            os.rename(f\"{picture_path}/flux.png\", new_image_name)\n",
        "            output_image.value = new_image_name\n",
        "            output_image.update(value=new_image_name)\n",
        "    if suicide_switch:\n",
        "        runtime.unassign()\n",
        "    return new_image_name\n",
        "\n",
        "with gr.Blocks(analytics_enabled=False) as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            positive_prompt = gr.Textbox(lines=3, interactive=True, value=\"Anime drawing, full body portrait, attractive 19-year-old Caucasian woman, long straight blonde hair, red lipstick, white button-up blouse, black neck tie, black suspenders, tan suit jacket, tan shorts, tan pantyhose, tan flat shoes, smiling, sitting in a recliner, legs crossed\", label=\"Prompt\")\n",
        "            #width = gr.Slider(minimum=256, maximum=2048, value=1024, step=16, label=\"width\")\n",
        "            #height = gr.Slider(minimum=256, maximum=2048, value=1024, step=16, label=\"height\")\n",
        "            with gr.Row():\n",
        "                #wh_ratio = gr.Textbox(lines=1, interactive=True, value=\"4:7\", label=\"width:height ratio\")\n",
        "                wh_ratio = gr.Dropdown([\"1:1\",\"2:1\",\"3:2\",\"4:3\",\"5:3\",\"7:4\",\"9:7\",\"16:9\",\"21:11\",\"17:15\"], value=\"7:4\", label=\"width:height ratio\")\n",
        "                orientation = gr.Dropdown([\"portrait\", \"landscape\"], label=\"orientation\", value=\"portrait\")\n",
        "                quantity = gr.Slider(minimum=1, maximum=10, value=1, step=1, label=\"quantity per prompt\")\n",
        "                suicide_switch = gr.Checkbox(value=False, label=\"Disconnect and delete runtime when done.\")\n",
        "                #width, height = optimal_dimensions(wh_ratio.value)\n",
        "            seed = gr.Slider(minimum=0, maximum=18446744073709551615, value=0, step=1, label=\"seed (0=random)\")\n",
        "            steps = gr.Slider(minimum=4, maximum=50, value=20, step=1, label=\"steps\")\n",
        "            guidance = gr.Slider(minimum=0, maximum=20, value=3.5, step=0.5, label=\"guidance\")\n",
        "            lora_strength_model = gr.Slider(minimum=0, maximum=1, value=1.0, step=0.1, label=\"lora_strength_model\")\n",
        "            lora_strength_clip = gr.Slider(minimum=0, maximum=1, value=1.0, step=0.1, label=\"lora_strength_clip\")\n",
        "            sampler_name = gr.Dropdown([\"euler\", \"heun\", \"heunpp2\", \"heunpp2\", \"dpm_2\", \"lms\", \"dpmpp_2m\", \"ipndm\", \"deis\", \"ddim\", \"uni_pc\", \"uni_pc_bh2\"], label=\"sampler_name\", value=\"euler\")\n",
        "            scheduler = gr.Dropdown([\"normal\", \"sgm_uniform\", \"simple\", \"ddim_uniform\"], label=\"scheduler\", value=\"simple\")\n",
        "            generate_button = gr.Button(\"Generate\")\n",
        "        with gr.Column():\n",
        "            output_image = gr.Image(label=\"Generated image\", interactive=False)\n",
        "\n",
        "    #generate_button.click(fn=generate, inputs=[positive_prompt, width, height, seed, steps, sampler_name, scheduler, guidance, lora_strength_model, lora_strength_clip], outputs=output_image)\n",
        "    generate_button.click(fn=generate_wrapper, inputs=[positive_prompt, quantity, wh_ratio, orientation, seed, steps, sampler_name, scheduler, guidance, lora_strength_model, lora_strength_clip], outputs=output_image)\n",
        "\n",
        "demo.queue().launch(inline=False, share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}