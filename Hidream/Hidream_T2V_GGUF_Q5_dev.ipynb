{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ljkrajewski/jupyter_notebooks/blob/main/Hidream/Hidream_T2V_GGUF_Q5_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hidream_GGUF_Q5-dev for Text to Image Generation**\n",
        "- Derived from https://github.com/Isi-dev/Google-Colab_Notebooks/blob/main/Hidream_T2V_GGUF_Q5.ipynb\n",
        "\n",
        "- The full version is much better than the others, but it took almost 21 minutes to generate an image with the default settings on the free T4 GPU."
      ],
      "metadata": {
        "id": "qOMU9X8Yi19t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "982f7zyAZzdc",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0914818-76aa-4dc3-fc1b-9138a9e1e46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installation completed with status:\n",
            "- All pip packages installed successfully\n",
            "- apt packages installed successfully\n",
            "Downloading hidream-i1-dev-Q5_0.gguf... Done!\n",
            "Downloading clip_g_hidream.safetensors... Done!\n",
            "Downloading clip_l_hidream.safetensors... Done!\n",
            "Downloading llama-q2_k.gguf... Done!\n",
            "Downloading t5-v1_1-xxl-encoder-Q6_K.gguf... Done!\n",
            "Downloading ae.safetensors... Done!\n",
            "✅ Environment Setup Complete!\n"
          ]
        }
      ],
      "source": [
        "# @title Setup Environment\n",
        "%cd /content\n",
        "!pip install torch==2.6.0 torchvision==0.21.0\n",
        "\n",
        "from IPython.display import clear_output\n",
        "# !pip install -q torchsde einops diffusers accelerate xformers\n",
        "# !git clone https://github.com/comfyanonymous/ComfyUI\n",
        "!git clone https://github.com/Isi-dev/ComfyUI\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone --branch forHidream https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "%cd /content/ComfyUI\n",
        "# !apt -y install -qq aria2\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "\n",
        "def install_pip_packages():\n",
        "    packages = [\n",
        "        'torchsde',\n",
        "        # 'av',\n",
        "        'diffusers',\n",
        "        # 'transformers',\n",
        "        'xformers==0.0.29.post2',\n",
        "        'accelerate',\n",
        "        # 'omegaconf',\n",
        "        # 'tqdm',\n",
        "        # 'librosa',\n",
        "        'einops'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            # Run pip install silently (using -q)\n",
        "            subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
        "                check=True,\n",
        "                capture_output=True\n",
        "            )\n",
        "            print(f\"✓ {package} installed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ Error installing {package}: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "def install_apt_packages():\n",
        "    packages = ['aria2']\n",
        "\n",
        "    try:\n",
        "        # Run apt install silently (using -qq)\n",
        "        subprocess.run(\n",
        "            ['apt-get', '-y', 'install', '-qq'] + packages,\n",
        "            check=True,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"✓ apt packages installed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Error installing apt packages: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "# Run installations\n",
        "print(\"Installing pip packages...\")\n",
        "install_pip_packages()\n",
        "clear_output()  # Clear the pip installation output\n",
        "\n",
        "print(\"Installing apt packages...\")\n",
        "install_apt_packages()\n",
        "clear_output()  # Clear the apt installation output\n",
        "\n",
        "print(\"Installation completed with status:\")\n",
        "print(\"- All pip packages installed successfully\" if '✗' not in install_pip_packages.__code__.co_consts else \"- Some pip packages had issues\")\n",
        "print(\"- apt packages installed successfully\" if '✗' not in install_apt_packages.__code__.co_consts else \"- apt packages had issues\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import os\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "\n",
        "from nodes import (\n",
        "    KSampler,\n",
        "    VAEDecode,\n",
        "    VAELoader,\n",
        "    CLIPTextEncode,\n",
        "    SaveImage\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import (\n",
        "    UnetLoaderGGUF,\n",
        "    QuadrupleCLIPLoaderGGUF\n",
        ")\n",
        "from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
        "from comfy_extras.nodes_sd3 import EmptySD3LatentImage\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Colab-optimized download with aria2c\n",
        "\n",
        "    Args:\n",
        "        url: Download URL\n",
        "        dest_dir: Target directory (will be created if needed)\n",
        "        filename: Optional output filename (defaults to URL filename)\n",
        "        silent: If True, suppresses all output (except errors)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create destination directory\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Set filename if not specified\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]  # Remove URL parameters\n",
        "\n",
        "        # Build command\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "\n",
        "        # Add silent flags if requested\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        # Run download\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "hidream_version = \"dev\"\n",
        "\n",
        "hidream_model = \"hidream-i1-dev-Q5_0.gguf\"\n",
        "\n",
        "model_download(\"https://huggingface.co/city96/HiDream-I1-Dev-gguf/resolve/main/hidream-i1-dev-Q5_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "clip_g = model_download(\"https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/text_encoders/clip_g_hidream.safetensors\", \"/content/ComfyUI/models/text_encoders\")\n",
        "clip_l = model_download(\"https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/text_encoders/clip_l_hidream.safetensors\", \"/content/ComfyUI/models/text_encoders\")\n",
        "llama = model_download(\"https://huggingface.co/calcuis/hidream-gguf/resolve/main/llama-q2_k.gguf\", \"/content/ComfyUI/models/text_encoders\")\n",
        "t5xxl = model_download(\"https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf/resolve/main/t5-v1_1-xxl-encoder-Q6_K.gguf\", \"/content/ComfyUI/models/text_encoders\")\n",
        "model_download(\"https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/vae/ae.safetensors\", \"/content/ComfyUI/models/vae\")\n",
        "\n",
        "# Initialize nodes\n",
        "unet_loader = UnetLoaderGGUF()\n",
        "model_sampling = ModelSamplingSD3()\n",
        "clip_loader = QuadrupleCLIPLoaderGGUF()\n",
        "clip_encode_positive = CLIPTextEncode()\n",
        "clip_encode_negative = CLIPTextEncode()\n",
        "vae_loader = VAELoader()\n",
        "empty_latent_image = EmptySD3LatentImage()\n",
        "ksampler = KSampler()\n",
        "vae_decode = VAEDecode()\n",
        "save_image = SaveImage()\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def generate_image(\n",
        "    #positive_prompt: str = \"anime girl with massive fennec ears and a big fluffy fox tail with long wavy blonde hair and blue eyes wearing a pink sweater a large oversized black winter coat and a long blue maxi skirt and large winter boots and a red scarf and large gloves sitting in a sled sledding fast down a snow mountain\",\n",
        "    context_prompt: str = \"\",\n",
        "    details_prompt: str = \"\",\n",
        "    text_prompt: str = \"\",\n",
        "    main_prompt: str = \"anime girl with massive fennec ears and a big fluffy fox tail with long wavy blonde hair and blue eyes wearing a pink sweater a large oversized black winter coat and a long blue maxi skirt and large winter boots and a red scarf and large gloves sitting in a sled sledding fast down a snow mountain\",\n",
        "    negative_prompt: str = \"bad ugly jpeg artifacts\",\n",
        "    width: int = 1024,\n",
        "    height: int = 1024,\n",
        "    seed: int = 0,\n",
        "    steps: int = 28,\n",
        "    cfg_scale: float = 1.0,\n",
        "    sampler_name: str = \"lcm\",\n",
        "    scheduler: str = \"simple\",\n",
        "    shift: float = 6.0\n",
        "):\n",
        "    with torch.inference_mode():\n",
        "        print(\"Loading CLIP models...\")\n",
        "        clip = clip_loader.load_clip(\n",
        "            clip_g,\n",
        "            clip_l,\n",
        "            t5xxl,\n",
        "            llama\n",
        "        )[0]\n",
        "\n",
        "        print(\"Encoding prompts...\")\n",
        "        #positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        positive_prompts = (context_prompt, details_prompt, text_prompt, main_prompt)\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompts)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Creating empty latent...\")\n",
        "        empty_latent = empty_latent_image.generate(width, height, 1)[0]\n",
        "\n",
        "        print(\"Loading UNet model...\")\n",
        "        model = unet_loader.load_unet(hidream_model)[0]\n",
        "        model = model_sampling.patch(model, shift)[0]\n",
        "\n",
        "        print(\"Generating Image...\")\n",
        "        sampled = ksampler.sample(\n",
        "            model=model,\n",
        "            seed=seed,\n",
        "            steps=steps,\n",
        "            cfg=cfg_scale,\n",
        "            sampler_name=sampler_name,\n",
        "            scheduler=scheduler,\n",
        "            positive=positive,\n",
        "            negative=negative,\n",
        "            latent_image=empty_latent\n",
        "        )[0]\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = vae_loader.load_vae(\"ae.safetensors\")[0]\n",
        "\n",
        "        try:\n",
        "            print(\"Decoding image...\")\n",
        "            decoded = vae_decode.decode(vae, sampled)[0]\n",
        "\n",
        "            del vae\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(\"Saving image...\")\n",
        "            output_path = save_image.save_images(decoded, \"ComfyUI\")[\"ui\"][\"images\"][0][\"filename\"]\n",
        "            full_path = f\"/content/ComfyUI/output/{output_path}\"\n",
        "\n",
        "            from IPython.display import display, Image\n",
        "            display(Image(filename=full_path))\n",
        "\n",
        "            return full_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during decoding/saving: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_memory()\n",
        "\n",
        "print(\"✅ Environment Setup Complete!\")\n",
        "\n",
        "# Example usage:\n",
        "# generate_image()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Subroutine Definitions\n",
        "\n",
        "import math\n",
        "\n",
        "def add_ai_metadata(image_path, prompt, seed, steps, guidance, sampler_name, scheduler, lora_strength_model, lora_strength_clip):\n",
        "    \"\"\"\n",
        "    Adds metadata related to a stable diffusion image generation to a PNG image file.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the PNG image file.\n",
        "        prompt: The text prompt used for image generation.\n",
        "        seed: The random seed used for image generation.\n",
        "        steps: The number of denoising steps.\n",
        "        guidance: The classifier-free guidance scale (cfg_scale).\n",
        "        sampler_name: The name of the sampler used.\n",
        "        scheduler: The scheduler used.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        png_info = PngInfo()\n",
        "\n",
        "        png_info.add_text(\"Model\", \"hidream-i1-dev-Q5_0.gguf\")\n",
        "        png_info.add_text(\"Prompt\", prompt)\n",
        "        png_info.add_text(\"Seed\", str(seed))\n",
        "        png_info.add_text(\"Steps\", str(steps))\n",
        "        png_info.add_text(\"Guidance\", str(guidance))\n",
        "        png_info.add_text(\"Sampler\", sampler_name)\n",
        "        png_info.add_text(\"Scheduler\", scheduler)\n",
        "        img.save(image_path, pnginfo=png_info)\n",
        "        print(f\"Metadata added successfully to {image_path}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def optimal_dimensions(wh_ratio):\n",
        "    \"\"\"\n",
        "    Calculates optimal dimensions for Stable Diffusion.\n",
        "\n",
        "    Args:\n",
        "      wh_ratio (str): Width-to-height ratio in the format \"width:height\".\n",
        "\n",
        "    Returns:\n",
        "      A tuple of (new_width, new_height) representing the optimal dimensions.\n",
        "    \"\"\"\n",
        "    sw, sh = wh_ratio.split(':')\n",
        "    w, h = int(sw), int(sh)\n",
        "    c = math.sqrt(1024**2 / (w * h))\n",
        "    new_width = int(((w * c) // 16) * 16)\n",
        "    new_height = int(((h * c) // 16) * 16)\n",
        "    #print(f\"Optimal dimensions: {new_width}x{new_height}\")\n",
        "    return new_width, new_height"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Wob_4WNO4CB1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate Image (Dev Version)\n",
        "# @markdown Prompts should be around 128 tokens (~10-25 words), 248 tokens max. <br><br>\n",
        "# @markdown **Positive prompts:**\n",
        "main_prompt = \"A 19-year-old Caucasian woman with long, straight blonde hair sits in a recliner with legs crossed, in a cozy room, anime style, wearing a white button-up blouse, black neck tie, black suspenders, tan suit jacket, tan shorts, tan pantyhose, and tan flat shoes, with red lipstick and a warm smile.\" # @param {\"type\":\"string\"}\n",
        "details_prompt = \"Long, straight blonde hair, red lipstick, white button-up blouse, black tie, black suspenders, tan suit jacket, tan shorts, tan pantyhose, tan flat shoes, detailed facial features.\" # @param {\"type\":\"string\"}\n",
        "context_prompt = \"Cozy room with a recliner, soft lighting, warm ambiance, full-body perspective.\" # @param {\"type\":\"string\"}\n",
        "text_prompt = \"Clear textural details on clothing, precise stitching on blouse and tie.\" # @param {\"type\":\"string\"}\n",
        "#negative_prompt = \"bad ugly jpeg artifacts blurry, extra limbs, distorted face, bad anatomy, low resolution, out of frame, cropped, low detail, unrealistic proportions, artifacts, dull\" # @param {\"type\":\"string\"}\n",
        "negative_prompt = \"\"\n",
        "# @markdown ----\n",
        "#width = 1440 # @param {\"type\":\"integer\", \"min\":512, \"max\":2048}\n",
        "#height = 720 # @param {\"type\":\"integer\", \"min\":512, \"max\":2048}\n",
        "aspect_ratio = \"3:2\" # @param [\"1:1\",\"2:1\",\"3:2\",\"4:3\",\"5:3\",\"7:4\",\"9:7\",\"16:9\",\"21:11\",\"17:15\"]\n",
        "orientation = \"portrait\" # @param [\"portrait\", \"landscape\"]\n",
        "seed = 0 # @param {\"type\":\"integer\"}\n",
        "steps = 28 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n",
        "cfg_scale = 1.0 # @param {\"type\":\"number\", \"min\":0.1, \"max\":20.0}\n",
        "sampler_name = \"euler\" # @param [\"lcm\", \"uni_pc\", \"euler\", \"dpmpp_2m\", \"ddim\", \"lms\"]\n",
        "scheduler = \"beta\" # @param [\"simple\", \"normal\", \"karras\", \"exponential\", \"beta\", \"ddim_uniform\", \"sgm_uniform\"]\n",
        "shift = 6.0 # @param {\"type\":\"number\", \"min\":0.0, \"max\":10.0}\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "if orientation == \"portrait\":\n",
        "    height, width = optimal_dimensions(aspect_ratio)\n",
        "else:\n",
        "    width, height = optimal_dimensions(aspect_ratio)\n",
        "print(f\"Orientation: {orientation}\")\n",
        "print(f\"Dimensions: {width}x{height}\")\n",
        "\n",
        "# Generate the image\n",
        "output_path = generate_image(\n",
        "    #positive_prompt=positive_prompt,\n",
        "    context_prompt=context_prompt,\n",
        "    details_prompt=details_prompt,\n",
        "    text_prompt=text_prompt,\n",
        "    main_prompt=main_prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=width,\n",
        "    height=height,\n",
        "    seed=seed,\n",
        "    steps=steps,\n",
        "    cfg_scale=cfg_scale,\n",
        "    sampler_name=sampler_name,\n",
        "    scheduler=scheduler,\n",
        "    shift=shift\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C-Mt6P2DZ_U9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93d93a2d-2d2f-40be-bc6a-e816c4f7332f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using seed: 785644970\n",
            "Orientation: portrait\n",
            "Dimensions: 832x1248\n",
            "Loading CLIP models...\n",
            "Encoding prompts...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'replace'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2090596299.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Generate the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m output_path = generate_image(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m#positive_prompt=positive_prompt,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcontext_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-515661045.py\u001b[0m in \u001b[0;36mgenerate_image\u001b[0;34m(context_prompt, details_prompt, text_prompt, main_prompt, negative_prompt, width, height, seed, steps, cfg_scale, sampler_name, scheduler, shift)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m#positive = clip_encode_positive.encode(clip, positive_prompt)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mpositive_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontext_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetails_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mpositive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_encode_positive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_encode_negative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ComfyUI/nodes.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, clip, text)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR: clip input is invalid: None\\n\\nIf the clip is from a checkpoint loader node your checkpoint does not contain a valid clip or text encoder model.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_from_tokens_scheduled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ComfyUI/comfy/sd.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, return_word_ids, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_word_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_with_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_word_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_hooks_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ComfyUI/comfy/text_encoders/hidream.py\u001b[0m in \u001b[0;36mtokenize_with_weights\u001b[0;34m(self, text, return_word_ids, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_with_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_word_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"g\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_with_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_word_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"l\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_with_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_word_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mt5xxl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt5xxl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_with_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_word_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ComfyUI/comfy/sd1_clip.py\u001b[0m in \u001b[0;36mtokenize_with_weights\u001b[0;34m(self, text, return_word_ids, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m         '''\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mescape_important\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0mparsed_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ComfyUI/comfy/sd1_clip.py\u001b[0m in \u001b[0;36mescape_important\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mescape_important\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\0\\1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\(\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\0\\2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'replace'"
          ]
        }
      ]
    }
  ]
}